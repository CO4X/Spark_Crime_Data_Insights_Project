# ğŸ“Š Spark Crime Data Insights Project

## ğŸ“ Description du projet

Ce projet a pour objectif de manipuler, nettoyer et analyser le dataset **Crimes in Chicago (2001 - prÃ©sent)** Ã  lâ€™aide de **PySpark** dans Google Colab.
Il sâ€™inscrit dans le cadre de mon **autoformation Big Data et Spark**, et vise Ã  explorer un jeu de donnÃ©es massif de plus de **8 millions de lignes**.

Lâ€™objectif principal est de mettre en pratique les fonctionnalitÃ©s de PySpark, depuis les opÃ©rations de base (nettoyage, typage, filtrage) jusquâ€™Ã  des analyses plus avancÃ©es (fenÃªtres, SQL, UDF, pivot tables).

---

## ğŸ“‚ Dataset utilisÃ©

* **Nom** : Crimes â€“ 2001 to Present
* **Source officielle** : [data.cityofchicago.org](https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-Present/ijzp-q8t2)
* **Taille** : ~8 millions de lignes, 22 colonnes
* **Format** : CSV â†’ transformÃ© en Parquet pour une meilleure performance

---

## âš™ï¸ Installation & Configuration

### 1. Connexion Google Colab â†” Google Drive

Montage du Drive pour accÃ©der aux donnÃ©es et sauvegarder les rÃ©sultats.

### 2. Installation des dÃ©pendances

* **PySpark** : moteur de traitement distribuÃ©
* **Findspark** : intÃ©gration Spark â†” Python

### 3. CrÃ©ation dâ€™une SparkSession

Point dâ€™entrÃ©e de toute application Spark avec un nom personnalisÃ© pour ce projet.

---

## ğŸ—ï¸ Ã‰tapes du projet

### 1ï¸âƒ£ Ingestion des donnÃ©es

* Lecture du fichier CSV brut
* VÃ©rification du schÃ©ma et aperÃ§u des premiÃ¨res lignes

### 2ï¸âƒ£ Nettoyage & Typage

* Conversion des colonnes string â†’ types appropriÃ©s (`IntegerType`, `BooleanType`, `DoubleType`, `TimestampType`)
* Exemple :

  ```python
  df = df.withColumn("Date", to_timestamp(col("Date"), "MM/dd/yyyy hh:mm:ss a"))
  ```

### 3ï¸âƒ£ Exploration des donnÃ©es

* Affichage du nombre total de lignes et colonnes
* AperÃ§u de certaines colonnes clÃ©s (`Date`, `Primary Type`, `Arrest`, `Location`)
* Ajout dâ€™une colonne constante `Dataset_Version`
* VÃ©rification du nombre de types de crimes distincts

### 4ï¸âƒ£ Analyses simples

* Nombre de crimes par type (`groupBy + count`)
* Nombre de crimes par lieu (`groupBy + orderBy`)
* Pourcentage de crimes menant Ã  une arrestation

### 5ï¸âƒ£ Analyses avancÃ©es

* **Pivot tables** : Arrestations par annÃ©e
* **Window functions** : dernier crime enregistrÃ© par district
* **Valeurs manquantes** : dÃ©tection colonne par colonne

### 6ï¸âƒ£ SQL avec Spark

* CrÃ©ation dâ€™une **vue temporaire** du DataFrame
* RequÃªtes SQL pour analyser les crimes menant Ã  des arrestations par type et par annÃ©e

### 7ï¸âƒ£ UDF (User Defined Function)

* CrÃ©ation dâ€™une fonction Python pour classifier les crimes (`Vol`, `Agression`, `Autre`)
* Transformation en UDF et ajout dâ€™une colonne `Crime_Category`

### 8ï¸âƒ£ Sauvegarde des rÃ©sultats

* Export du DataFrame nettoyÃ© et enrichi en format **Parquet**
* Sauvegarde dans **Google Drive** pour rÃ©utilisation ultÃ©rieure

---

## ğŸ“Š RÃ©sultats principaux

* Exploration dâ€™un dataset de **8 millions de lignes**
* Analyses descriptives sur les types et lieux de crimes
* Calculs statistiques (taux dâ€™arrestation, Ã©volution par annÃ©e)
* Mise en place dâ€™outils avancÃ©s : **fenÃªtres, pivots, SQL, UDF**
* Sauvegarde optimisÃ©e en Parquet

---

## ğŸš€ Perspectives

Ce projet constitue une **base solide** pour lâ€™apprentissage de PySpark.
Lâ€™Ã©tape suivante sera :

* Dâ€™Ã©largir les **analyses** avec des fonctions statistiques plus poussÃ©es
* De couvrir la totalitÃ© des **fonctions PySpark** (DataFrame API, RDD API, MLlib, GraphFrames, etc.)
* Dâ€™intÃ©grer des **dashboards** pour la visualisation interactive (ex. Power BI, Databricks, ou Plotly).
